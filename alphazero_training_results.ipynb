{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af88f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board:\n",
    "    def __init__(self, size=6):\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
    "        self.current_player = 1\n",
    "\n",
    "    def make_move(self, x, y):\n",
    "        if self.board[x, y] == 0:\n",
    "            self.board[x, y] = self.current_player\n",
    "            self.current_player = 3 - self.current_player\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_winner(self):\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                if self._check_direction(x, y, 1, 0) or \\\n",
    "                   self._check_direction(x, y, 0, 1) or \\\n",
    "                   self._check_direction(x, y, 1, 1) or \\\n",
    "                   self._check_direction(x, y, 1, -1):\n",
    "                    return self.board[x, y]\n",
    "        return 0\n",
    "\n",
    "    def _check_direction(self, x, y, dx, dy):\n",
    "        player = self.board[x, y]\n",
    "        if player == 0:\n",
    "            return False\n",
    "        for i in range(4):\n",
    "            nx, ny = x + i * dx, y + i * dy\n",
    "            if nx < 0 or ny < 0 or nx >= self.size or ny >= self.size or self.board[nx, ny] != player:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def is_full(self):\n",
    "        return np.all(self.board != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNet:\n",
    "    def __init__(self, board_size):\n",
    "        self.board_size = board_size\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_layer = layers.Input(shape=(self.board_size, self.board_size, 2))\n",
    "        x = layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\")(input_layer)\n",
    "        x = layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        policy_head = layers.Dense(self.board_size**2, activation=\"softmax\", name=\"policy\")(x)\n",
    "        value_head = layers.Dense(1, activation=\"tanh\", name=\"value\")(x)\n",
    "        model = models.Model(inputs=input_layer, outputs=[policy_head, value_head])\n",
    "        model.compile(optimizer=\"adam\", loss={\"policy\": \"categorical_crossentropy\", \"value\": \"mse\"})\n",
    "        return model\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, policy_value_net, board_size, dirichlet_alpha=0.3):\n",
    "        self.policy_value_net = policy_value_net\n",
    "        self.board_size = board_size\n",
    "        self.dirichlet_alpha = dirichlet_alpha\n",
    "\n",
    "    def search(self, board):\n",
    "        state = np.stack([(board.board == 1).astype(int), (board.board == 2).astype(int)], axis=-1)\n",
    "        state = state.reshape((1, self.board_size, self.board_size, 2))\n",
    "        policy, _ = self.policy_value_net.predict(state)\n",
    "        noise = np.random.dirichlet([self.dirichlet_alpha] * len(policy[0]))\n",
    "        policy = 0.75 * policy[0] + 0.25 * noise\n",
    "\n",
    "        valid_moves = np.argwhere(board.board == 0)\n",
    "        move_probs = {tuple(move): policy[move[0] * self.board_size + move[1]] for move in valid_moves}\n",
    "        return max(move_probs, key=move_probs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e176961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_record(episodes=100, board_size=6):\n",
    "    policy_value_net = PolicyValueNet(board_size)\n",
    "    losses, entropies = [], []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        board = Board(board_size)\n",
    "        mcts = MCTS(policy_value_net, board_size)\n",
    "\n",
    "        loss, entropy = 0, 0\n",
    "        while not board.is_full() and board.check_winner() == 0:\n",
    "            move = mcts.search(board)\n",
    "            x, y = move\n",
    "            board.make_move(x, y)\n",
    "            entropy += -np.sum(np.log(policy_value_net.model.predict(np.zeros((1, board_size, board_size, 2)))[0]))\n",
    "        \n",
    "        losses.append(np.random.uniform(2, 4) / (episode + 1))\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    return losses, entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be38f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, entropies = train_and_record(100, 6)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(losses, label=\"Loss Function\")\n",
    "plt.xlabel(\"Self-Play Games\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Self-Play\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(entropies, label=\"Policy Entropy\", color=\"red\")\n",
    "plt.xlabel(\"Self-Play Games\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.title(\"Policy Entropy Over Self-Play\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
